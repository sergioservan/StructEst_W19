{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import everything needed:\n",
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "import requests\n",
    "import matplotlib.pyplot as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import math as math\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from scipy.stats import gamma\n",
    "import scipy.special as spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open and load data:\n",
    "\n",
    "data = pd.read_csv('MacroSeries.txt', sep=\",\", header=None, names=[\"c\",\"k\",\"w\",\"r\"]) \n",
    "#We put it names to the column so it is easier to call them later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we compute (back) \"z\" using eqn (3). Note that it depends on w, k and alfa:\n",
    "def z_back_3(w,k,alfa):\n",
    "    z = np.log(w/(((k)**alfa)*(1-alfa)))\n",
    "    return z\n",
    "\n",
    "#We define a criterion function for \"z\" knowing that it follows a normal distribution\n",
    "def crit_z3(params,data):\n",
    "    alfa, rho, mu, sigma = params\n",
    "    z = z_back_3(data['w'],data['k'],alfa)\n",
    "    z_lag = z.shift(1)\n",
    "    z_lag[0] = mu   #According to whatis given in the question\n",
    "    \n",
    "    #\"z\" follows a normal distribution. We are computing the log-likelihood:\n",
    "    log_lik_val = - np.log(sigma) - 0.5*np.log(2 * np.pi) - (z - (rho*z_lag+(1-rho)*mu))**2 / (2 * sigma**2)\n",
    "    \n",
    "    return -np.mean(log_lik_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alfa_MLE_a= 0.45748197895249465\n",
      "rho_MLE_a= 0.7205109687252601\n",
      "mu_MLE_a= 9.523194350515697\n",
      "sigma_MLE_a= 0.09199596967853813\n",
      "Log-likelihood= -0.9670690801228998\n",
      "Variance-Covariance matrix\n",
      "  [[ 2.19901603e+01 -4.40992844e+01 -3.17083758e+02 -7.23370956e-01]\n",
      " [-4.40992844e+01  9.64729330e+01  6.38161496e+02  1.64509107e+00]\n",
      " [-3.17083758e+02  6.38161496e+02  4.57294356e+03  1.04907487e+01]\n",
      " [-7.23370956e-01  1.64509107e+00  1.04907487e+01  3.47710374e-02]]\n"
     ]
    }
   ],
   "source": [
    "#MLE estimation of parameters:\n",
    "import scipy.optimize as opt\n",
    "\n",
    "#We begin with some guess for the initial values. Unlike Question 1, we are not given any suggestion to where to start the\n",
    "#values so we arbitrary choose them. (Note: Changing the initial values changes the optimized values)\n",
    "alfa_init = 0.1\n",
    "rho_init = 0.1\n",
    "mu_init = 0.1\n",
    "sigma_init = 1\n",
    "params_init = np.array([alfa_init, rho_init, mu_init, sigma_init])\n",
    "mle_args = data\n",
    "\n",
    "#We use the 'L_BFGS-B' method because is the only one that returns the inverse Hessian. We also put the bound so that\n",
    "#alfa, rho, mu and sigma are constrained according to what it is said in the question.\n",
    "results_cstr_a = opt.minimize(crit_z3, params_init, args=(mle_args), method='L-BFGS-B',\n",
    "                            bounds=((1e-10, 0.9999), (-0.9999, 0.9999),(1e-10, None), (1e-10, None)))\n",
    "alfa_MLE_a, rho_MLE_a, mu_MLE_a, sigma_MLE_a = results_cstr_a.x\n",
    "log_lik_val_a = crit_z3([alfa_MLE_a,rho_MLE_a,mu_MLE_a,sigma_MLE_a],data)\n",
    "inv_hess = results_cstr_a.hess_inv.todense()\n",
    "\n",
    "print('alfa_MLE_a=', alfa_MLE_a)\n",
    "print('rho_MLE_a=', rho_MLE_a)\n",
    "print('mu_MLE_a=', mu_MLE_a)\n",
    "print('sigma_MLE_a=', sigma_MLE_a)\n",
    "print('Log-likelihood=', log_lik_val_a)\n",
    "print('Variance-Covariance matrix\\n ', inv_hess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we compute (back) \"z\" using eqn (4). Note that it depends on r, k and alfa\n",
    "def z_back_4(r,k,alfa):\n",
    "    z = np.log(r/(((k)**(alfa-1))*(alfa)))\n",
    "    return z\n",
    "\n",
    "#Criterion function\n",
    "def crit_z4(params,data):\n",
    "    alfa, rho, mu, sigma = params\n",
    "    z = z_back_4(data['r'],data['k'],alfa)\n",
    "    z_lag = z.shift(1)\n",
    "    z_lag[0] = mu      #According to whatis given in the question\n",
    "    \n",
    "    #\"z\" follows a normal distribution. We are computing the log-likelihood::\n",
    "    log_lik_val = - np.log(sigma) - 0.5*np.log(2 * np.pi) - (z - (rho*z_lag+(1-rho)*mu))**2 / (2 * sigma**2)\n",
    "    \n",
    "    return -np.mean(log_lik_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alfa_MLE_b= 0.7018970334618823\n",
      "rho_MLE_b= 0.4800169691930428\n",
      "mu_MLE_b= 5.077378123134153\n",
      "sigma_MLE_b= 0.09204404228038665\n",
      "Log-likelihood= -0.9665373698056341\n",
      "Variance-Covariance matrix\n",
      "  [[ 7.01342559e+01 -8.61010410e+01 -1.20605820e+03  7.19879234e+00]\n",
      " [-8.61010410e+01  2.39903561e+02  1.47228032e+03 -6.66048300e+00]\n",
      " [-1.20605820e+03  1.47228032e+03  2.07404438e+04 -1.23911369e+02]\n",
      " [ 7.19879234e+00 -6.66048300e+00 -1.23911369e+02  8.33811000e-01]]\n"
     ]
    }
   ],
   "source": [
    "#MLE estimation of parameters:\n",
    "import scipy.optimize as opt\n",
    "\n",
    "#We begin with some guess for the initial values. We will use the same used in part (a). We could also use, for example, the\n",
    "#MLE estimates from part (a). (Note: Changing the initial values changes the optimized values)\n",
    "alfa_init = 0.1\n",
    "rho_init = 0.1\n",
    "mu_init = 0.1\n",
    "sigma_init = 1\n",
    "params_init = np.array([alfa_init, rho_init, mu_init, sigma_init])\n",
    "mle_args = data\n",
    "\n",
    "#We use the 'L_BFGS-B' method because is the only one that returns the inverse Hessian. We also put the bound so that\n",
    "#alfa, rho, mu and sigma are constrained according to what it is said in the question.\n",
    "results_cstr_b = opt.minimize(crit_z4, params_init, args=(mle_args), method='L-BFGS-B',\n",
    "                            bounds=((1e-10, 0.9999), (-0.9999, 0.9999),(1e-10, None), (1e-10, None)))\n",
    "alfa_MLE_b, rho_MLE_b, mu_MLE_b, sigma_MLE_b = results_cstr_b.x\n",
    "log_lik_val_b = crit_z4([alfa_MLE_b,rho_MLE_b,mu_MLE_b,sigma_MLE_b],data)\n",
    "inv_hess_b = results_cstr_b.hess_inv.todense()\n",
    "\n",
    "print('alfa_MLE_b=', alfa_MLE_b)\n",
    "print('rho_MLE_b=', rho_MLE_b)\n",
    "print('mu_MLE_b=', mu_MLE_b)\n",
    "print('sigma_MLE_b=', sigma_MLE_b)\n",
    "print('Log-likelihood=', log_lik_val_b)\n",
    "print('Variance-Covariance matrix\\n ', inv_hess_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z* is  5.073061810972623\n",
      "Probability r>1 is  1.0\n"
     ]
    }
   ],
   "source": [
    "#Initial values for 'r' and 'k'\n",
    "r = 1\n",
    "k = 7500000\n",
    "\n",
    "#We compute 'z*' according to eqn (4). It is the same formula que use in part (b). We name the this variable z_target\n",
    "#We need to employ a value for alfa. Since we are using the back out procedure using eqn (4), we decide to use the ML \n",
    "#estimates of alfa from part (b)\n",
    "z_target = np.log(r/(((k)**(alfa_MLE_b-1))*(alfa_MLE_b)))\n",
    "print('z* is ',z_target)\n",
    "\n",
    "#Computing the probability r > 1 is equivalent to computting the probability that z > z*. Moreover we know the mean and\n",
    "#sigma of z, so we can compute the proability using the CDF of a normal distribution. Note that z_t-1 = 10:\n",
    "prob = 1 - sts.norm.cdf(z_target, loc = rho_MLE_b*10 + (1-rho_MLE_b)*mu_MLE_b, scale = sigma_MLE_b)\n",
    "print('Probability r>1 is ',prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
